
Краткий репорт
Задача заключалась в автоматической классификации миллионов наименований ТРУ при отсутствии готовой разметки. Тексты оказались очень короткими, шумными, с большим количеством аббревиатур, артикулов, спецобозначений и смешанной русско-английской лексики. В таких условиях обычные модели не могут обучаться напрямую — им просто нечего «зацепить» в исходном виде данных. Поэтому мы использовали каскадную архитектуру из трёх уровней, где каждый слой решает свой класс проблем.

Первый слой — rule-based классификатор. Он строится на жёстких правилах и системе якорных слов, заранее извлечённых из частотного анализа корпуса. Этот слой дал 75.4% покрытия всей выборки. Для повторяющихся шаблонов, стандартных услуг и типовых формулировок он работает надёжно, но дальнейшее расширение правил приводит к снижению качества и ломает масштабируемость. Поэтому rule-based выступает только как точная база для псевдоразметки.

Второй слой — модель TF-IDF + SVM, обученная на очищенной части псевдолейблов. Она расширяет покрытие ещё на +2.23% и показывает высокие метрики относительно rules: Accuracy 0.9851, F1-macro 0.9682, F1-weighted 0.9854. SVM хорошо подходит под разреженные текстовые матрицы и работает быстро даже на больших объёмах данных, что важно для нашего датасета.

Третий слой — BERT (RuBERT). Он нужен для сложных случаев, где важен контекст, формулировки нестандартные, а смысл скрыт между словами. На момент обучения около 77.63% выборки было псевдоразмечено, и BERT должен выступать финальным уточняющим классификатором для оставшихся трудных примеров. Обучение оказалось длительным (около 11 часов на 3 эпохах), поэтому модель не успела дообучиться, и финальные метрики пока недоступны. Также не был завершён итоговый интегрированный pipeline (FinalClassifier), объединяющий все три слоя.

Для всей системы мы разработали единый трёхуровневый препроцессинг: нормализацию текста, очистку от артикулов, кодов, дат и спецсимволов, а также лемматизацию (Natasha или pymorphy3). Такой препроцессинг делает текст стабильным и единообразным для правил, SVM и BERT, что критически важно при работе с миллионами строк.

В результате была построена масштабируемая многоуровневая система, способная автоматически разметить большую часть огромного корпуса ТРУ и служащая базой для дальнейшего улучшения качества классификации средствами современных моделей.


1) pip install -r requirements.txt
   (pip install natasha pymorphy3 razdel tqdm pandas numpy pyyaml)
2) python scripts/02_generate_pseudo_labels.py   проверь RAW_FILE = Path(r".csv") 
4) python scripts/07_generate_full_labels.py
 
